import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import GridSearchCV


# Load the dataset
data = pd.read_excel(r"C:\Users\91939\Downloads\Credit Card Defaulter Prediction.xlsx")

# Exploratory data analysis (EDA)
print("Data summary:")
print(data.describe())  # View summary statistics
print("Missing values:")
print(data.isnull().sum())  # Check for missing values

# Data cleaning (replace as needed based on your exploration)
#  ... (handle missing values, outliers, etc.)

# One-hot encode categorical variables
categorical_cols = ['SEX', 'EDUCATION', 'MARRIAGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']
data = pd.get_dummies(data, columns=categorical_cols, drop_first=True)

# Convert 'default' column to 0 and 1
data['default '] = data['default '].map({'N': 0, 'Y': 1})

# Split features and target variable
X = data.drop('default ', axis=1)
y = data['default ']

# Address class imbalance (if applicable)
if len(y.unique()) == 2 and y.value_counts().iloc[0] / y.value_counts().iloc[1] > 2:
    # Example: Use stratified shuffle split for imbalanced classes
    sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)
    for train_index, test_index in sss.split(X, y):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]
else:
    # Regular train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature engineering (optional)
# ... (create new features from existing ones)

# Hyperparameter tuning with Random Forest
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 8]
}

model = RandomForestClassifier(random_state=42)
grid_search = GridSearchCV(model, param_grid, cv=5, scoring='roc_auc')
grid_search.fit(X_train, y_train)
best_model = grid_search.best_estimator_
print("Best hyperparameters:", grid_search.best_params_)

# Model training
best_model.fit(X_train, y_train)

# Predict probabilities for test set
y_prob = best_model.predict_proba(X_test)[:, 1]

# Calculate ROC curve and AUC
fpr, tpr, thresholds = roc_curve(y_test, y_prob, pos_label=1)
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc="lower right")
plt.show()

# Calculate K-S statistic
ks_statistic = max(tpr - fpr)

# Plot K-S chart
plt.figure()
plt.plot(thresholds, 1 - fpr, label='1 - False Positive Rate (Specificity)')
plt.plot(thresholds, tpr, label='True Positive Rate (Sensitivity)')  # Added closing quotation mark here
plt.plot(thresholds, tpr - fpr, label='K-S Statistic')
plt.title('K-S Chart')
plt.xlabel('Threshold')
plt.ylabel('Rate/Statistic')
plt.legend()
plt.show()

print("Maximum K-S statistic:", ks_statistic)

